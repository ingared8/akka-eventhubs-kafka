main {

  appName = "akka-eventhubs-kafka"
  appName = ${?APP_NAME}

  logLevel = "debug"
  logLevel = ${?LOG_LEVEL}

  op = "READ_KAFKA"
  op = ${?OP}
}

eventhubs {

  // for publishing to eventhubs
  connStr = "CHANGE_ME"
  connStr = ${?EVENTHUBS_CONNSTR}

}

akka {

  loglevel = "DEBUG"
  loglevel = ${?AKKA_LOG_LEVEL}
  stdout-loglevel = "DEBUG"
  stdout-loglevel = ${?STDOUT_LOG_LEVEL}

  actor {
    provider = akka.actor.LocalActorRefProvider
    #provider = cluster
    #provider = remote
  }

  remote {
    enabled-transports = ["akka.remote.netty.tcp"]
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      #port = 2552
      port = 0
    }
  }

  cluster {
    seed-nodes = [
      "akka.tcp://ClusterSystem@127.0.0.1:2551",
      "akka.tcp://ClusterSystem@127.0.0.1:2552"
    ]
  }

}

// @see http://doc.akka.io/docs/akka/2.4.10/scala/logging.html
akka {
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = ${?AKKA_LOG_LEVEL}
  loglevel = "INFO"
}

eventhubs-1 {

  eventhub-react {

    // Connection settings can be retrieved from the Azure portal at https://portal.azure.com
    connection {

      // your-namespace.servicebus.windows.net
      eventHubEndpoint = ${EVENTHUBS_1_ENDPOINT}

      // eh1
      eventHubName = ${EVENTHUBS_1_NAME}

      // 0..n
      eventHubPartitions = ${EVENTHUBS_1_PARTITION_COUNT}

      // SAS policy name
      accessPolicy = ${EVENTHUBS_1_ACCESS_POLICY}

      // policy key
      accessKey = ${EVENTHUBS_1_ACCESS_KEY}
    }

    streaming {

      // "$Default" is predefined and is the typical scenario
      consumerGroup = "$Default"
      consumerGroup = ${?EVENTHUBS_1_CONSUMER_GROUP}

      // Value expressed as a duration, e.g. 3s, 3000ms, "3 seconds", etc.
      receiverTimeout = 3s
      receiverTimeout = ${?EVENTHUBS_1_RECEIVER_TIMEOUT}

      // How many messages to retrieve on each pull, max is 999
      receiverBatchSize = 300
      receiverBatchSize = ${?EVENTHUBS_1_RECEIVER_BATCH_SIZE}

      // Whether to retrieve information about the partitions while streaming events from Event Hub
      retrieveRuntimeInfo = true
    }

    checkpointing {

      // Checkpoints frequency (best effort), for each Event hub partition
      // Min: 1 second, Max: 1 minute
      frequency = 5s

      // How many messages to stream before saving the position, for each Event hub partition.
      // Since the stream position is saved in the Source, before the rest of the
      // Graph (Flows/Sinks), this provides a mechanism to replay buffered messages.
      // The value should be greater than receiverBatchSize
      countThreshold = 500
      countThreshold = ${?EVENTHUBS_1_COUNT_THRESHOLD}

      // Store a position if its value is older than this amount of time, ignoring the threshold.
      // For instance when the telemetry stops, this will force to write the last offset after some time.
      // Min: 1 second, Max: 1 hour. Value is rounded to seconds.
      timeThreshold = 10s

      storage {

        // Value expressed as a duration, e.g. 3s, 3000ms, "3 seconds", etc.
        rwTimeout = 5s

        // Supported types (not case sensitive): Cassandra, AzureBlob
        // backendType = "Cassandra"
        backendType = "AzureBlob"

        // If you use the same storage while processing multiple streams, you'll want
        // to use a distinct table/container/path in each job, to to keep state isolated
        namespace = "eventhub-react-checkpoints"
        namespace = ${?EVENTHUBS_1_CHECKPOINT_PATH}

        azureblob {
          // Time allowed for a checkpoint to be written, rounded to seconds (min 15, max 60)
          lease = 15s
          // Whether to use the Azure Storage Emulator
          useEmulator = false
          // Storage credentials
          protocol = "https"
          account = ${?EVENTHUBS_1_CHECKPOINT_ACCOUNT}
          key = ${?EVENTHUBS_1_CHECKPOINT_KEY}
        }

      }
    }
  }
}

kafka {

  parallelism = "4"
  parallelism = ${?PARALLELISM }

  bootstrap = "localhost:9092"
  bootstrap = ${?BOOTSTRAP}

  consumerGroup = "$Default"
  consumerGroup = ${?CONSUMER_GROUP}

  topic = "defaultTopic"
  topic = ${?TOPIC}

}

# Properties for akka.kafka.ConsumerSettings can be
# defined in this section or a configuration section with
# the same layout.
akka.kafka.consumer {
  # Tuning property of scheduled polls.
  poll-interval = 50ms

  # Tuning property of the `KafkaConsumer.poll` parameter.
  # Note that non-zero value means that blocking of the thread that
  # is executing the stage will be blocked.
  poll-timeout = 50ms

  # The stage will be await outstanding offset commit requests before
  # shutting down, but if that takes longer than this timeout it will
  # stop forcefully.
  stop-timeout = 30s

  # How long to wait for `KafkaConsumer.close`
  close-timeout = 20s

  # If offset commit requests are not completed within this timeout
  # the returned Future is completed `TimeoutException`.
  commit-timeout = 15s

  # If the KafkaConsumer can't connect to the broker the poll will be
  # aborted after this timeout. The KafkaConsumerActor will throw
  # org.apache.kafka.common.errors.WakeupException which will be ignored
  # until max-wakeups limit gets exceeded.
  wakeup-timeout = 3s

  # After exceeding maxinum wakeups the kafka will stop and the stage will fail.
  max-wakeups = 10

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the KafkaConsumerActor. Some blocking may occur.
  use-dispatcher = "akka.kafka.default-dispatcher"

  # Properties defined by org.apache.kafka.clients.kafka.ConsumerConfig
  # can be defined in this configuration section.
  kafka-clients {
    # Disable auto-commit by default
    enable.auto.commit = false
  }
}

